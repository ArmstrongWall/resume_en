<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<title>Johnny Wang | Robotics，Computer Vision</title>
<link href="css.css" rel="stylesheet" type="text/css" />
</head>

<body>
<div id="main">

	
		<div id="top-nav">
		
			<b>Ziqiang Wang</b>  <small>|Robotics Researcher</small>
			<a href="https://armstrongwall.github.io/resume_ch/"><img border="0"  height="5px"/> [中文版]</a>

		</div>

	
		<div id="header">
		
			<img src="images/header.png" alt="" width="770"/>
			
		</div>
		

	
		<div id="navigation" style="position: fixed">

			
			main menu
			
			<hr />
			
<a href="#Home" class="navigation">Home</a>
<a href="#Projects" class="navigation">Project Highlights</a>
<a href="#Publications" class="navigation">Publications</a>
<a href="#Rewards" class="navigation">Rewards</a>
		</div>
		

		<div id="content">
		
			<a name="Home"></a><h1>Welcome</h1>


    <p align="justify"> 
<img border="0" src="images/wzq.jpg" align="left" style="padding-right: 20px;padding-bottom: 20px"/>
		I'm a Computer Vision researcher with focus on robot state estimaton, SLAM, VIO , 3D Computer Vision and Reconstruction. 
		I finished my master at Tongji University in 2018, and since then am a researcher at Uisee in Shanghai China.
		On this website you can find my publications and associated videos, as well as links to open-source code and datasets.
</p><p> 
<b>Links: </b>
<a href="https://github.com/ArmstrongWall"><img border="0" src="images/gh.png" height="13px"/> GitHub</a>


<br/>
<b>Contact: </b>
<a href="mailto:jajuengel@gmail.com">1531651@tongji.edu.cn</a>

<br/>
</p>






<br /><br /><br /><br />

			<a name="Projects"></a><h1>Project Highlights</h1>

<h2>1.Stereo-VI-DSO:Direct Sparse Visual-Inertial Odometry with Stereo Cameras</h2>
<p align="justify"> We present <b>Stereo-VI-DSO</b>, a novel tightly-coupled
	approach for visual-inertial odometry, which jointly optimizes
	all the model parameters within the active window, including the
	IMU pose, velocity, biases, affine brightness parameters of all
	keyframes and the depth values of all selected pixels. The visual
	part of the system is integrated constraints from static stereo into
	the bundle adjustment pipeline of dynamic multi-view stereo,
	but unlike keypoint based systems it directly minimizes the
	photometric error. Fixed-baseline stereo corrects scale drift.
	IMU information is accumulated between keyframes using
	measurement pre-integration, and it is inserted into the
	optimization as an additional constraint between keyframes.
	Quantitative evaluation demonstrates that the proposed Stereo
	VI-DSO is superior to Stereo DSO both in terms of tracking
	accuracy and robustness. In addition, we introduce a simulation
	platform developed on Unreal Engine 4, it can output raw data
	of most sensors used in the field of autonomous driving. We
	evaluate our method with absolute ground-truth value base on
	simulation data.Paper and video can be seen at <a href="#Publications">Publications</a>[1]。We reconstructed the scene of our work zone on Unreal
	Engine 4 as shown in Fig 1.
	<br />
	<img border="0" src="images/s-vi-dso.png" align="middle"  width="500px" style="padding-right: 20px;padding-bottom: 20px"/>
	<br />
	Figure 1. Bottom: Example images from the simulation platform dataset:
	Strong motion, low illumination significant challenges for odometry
	estimation. Still our method is able to process all sequences with a rmse of
	less then 0.33m. Top: Reconstruction, estimated trajectory (bule). Top right
	is a topview in same road in Unreal Editor.
</p>


<h2>2.Wheeled Robots Path Planing and Tracking System Based on
	Monocular Visual SLAM</h2>
<p align="justify">
	Robots will work in different
	warehouse environments. In order to enable robots to perceive
	environment and plan path faster without modifying existing
	warehouses, we uses monocular camera to achieve an efficient
	robot integrated system. Mapping and path planning the two
	main tasks presented in this paper. The direct method visual
	odometry is applied to localize, and the 3D position of major
	obstacles in the environment is calculated. We describe the
	terrain with occupied grid map, the 3D points are projected onto
	the robot motion plane, thus accessibility of each grid is
	determined. Based on the terrain information, the optimized A*
	algorithm is used for path planning. Finally, according to
	localization and planning, we control the robot to track path. We
	also develop a path-tracking robot prototype. Simulation and
	experimental results verify the effectiveness and reliability of the
	proposed method.
	Paper and video can be seen at <a href="#Publications">Publications</a>[2][3]。We reconstructed the scene of our work zone as shown in Fig 2.
	<br />
	<img border="0" src="images/wheel.png" align="middle"  width="500px" style="padding-right: 20px;padding-bottom: 20px"/>
	<br />
	Figure 2: The environmental topographic map on the left shows an impassable area in yellow and an impassable area in green. On the right is a prototype robot.
</p>
<br />
<br />

<h2>3.Autobots based on Gmapping and ROS</h2>
<p align="justify">
	Two dimensional lidar RPLIDAR was installed on the robot of four-wheel forklift, and ROS Gmapping was used to complete the mapping and positioning functions. ROS 'slam_gmapping node will subscribe to the LaserScan node to get LaserScan data,
	Then calls the Rao - Blackwellized particle filter incremental grid map, map node to complete the building in a news release at the same time and output robot positioning data to the tf node. Gmapping is the open source 2D laser SLAM algorithm published by professor Giorgio at the university of freiburg in Germany in 2007.
	In the early years of SLAM, it had great significance and was widely used. Four-wheel forklift robot is used for cargo transportation in warehouse environment.
	We reconstructed the scene of our work zone as shown in Fig 3.
	<br />
	<img border="0" src="images/gmapping.png" align="middle"  width="500px" style="padding-right: 20px;padding-bottom: 20px"/>
	<br />
	Figure 3: The left picture is a two-dimensional Lidar odometer map, the middle picture is a forklift robot drawing, and the right picture is a robot entity.
</p>


<br /><br />
<br /><br />
<a name="Publications"></a><h1>Publications</h1>

<p>
	<img border="0" src="pdf_img/engel17phd.png" align="left" width="80px" style="padding-right: 10px;padding-bottom: 10px"/>
	<b>1.Direct Sparse Visual-Inertial Odometry with Stereo Cameras</b> (Ziqiang Wang, Chengcheng Guo), <i>No open,under review by IROS</i>, 2019.
	<br />
	<a href="https://github.com/ArmstrongWall/Supplementary_Material_to_Stereo_VI_DSO/blob/master/Supplementary%20Material%20to%20Direct%20Sparse%20Visual-Inertial%20Odometry%20with%20Stereo%20Cameras.pdf"><img border="0" src="images/pdf.png" height="13px"/> [Supplementary Material]</a>
	<a href="https://v.youku.com/v_show/id_XNDE1NTUzNzg0OA==.html?spm=a2h3j.8428770.3416059.1"><img border="0" src="images/yt.png" height="13px"/> [Video]</a>
</p>


<p align=justify">
	<img border="0" src="pdf_img/engel2016dso.jpg" align="left" width="80px" style="padding-right: 10px;padding-bottom: 0px"/>
	<b>2.Wheeled Robots Path Planing and Tracking System Based on Monocular Visual SLAM</b> (Ziqiang Wang, Hegen Xu, Youwen Wan), 2018.
	<br />
	<a href="https://arxiv.org/abs/1807.06303"><img border="0" src="images/pdf.png" height="13px"/> [pdf]</a>
	<a href="https://v.youku.com/v_show/id_XNDA5OTc5MDEzMg==.html?spm=a2hzp.8244740.0.0"><img border="0" src="images/yt.png" height="13px"/> [Video]</a>
	<a href="https://github.com/ArmstrongWall/lsd-slam"><img border="0" src="images/gh.png" height="13px"/> [code]</a>
</p>

<p>
	<img border="0" src="pdf_img/stumberg16exploration.png" align="left" width="80px" style="padding-right: 10px;padding-bottom: 20px"/>
	<b>3.Research and Implementation of Robot Path Planning Based on VSLAM</b> (Ziqiang Wang, Hegen Xu, Youwen Wan), In <i>International Conference on Electrical Engineering, Control and Robotics (EECR)</i>, 2017.
	<br />
	<a href="https://doi.org/10.1051/matecconf/201816006004"><img border="0" src="images/pdf.png" height="13px"/> [pdf]</a>
</p>
			<br /><br />
			<br /><br />
<a name="Rewards"></a><h1>Rewards</h1>

			<p>
				<b>1.“Freescale cup” National College Students Intelligent Car Competition ，National First Prize, 2013.</b>
				<a href="https://v.youku.com/v_show/id_XMzA4NzgzODg4MA==.html?spm=a2hzp.8244740.0.0"><img border="0" src="images/yt.png" height="13px"/> [video]</a>
				<br />
				<img border="0" src="images/rw3.jpg" align="middle"  width="400px" >
			</p>

			<br />

<p>
	<b>2.“Huawei cup” National Post-Graduate Mathematical Contest in Modeling ，National Second Prize, 2017.</b>
	<br />
	<img border="0" src="images/rw1.jpg" align="middle"  width="400px" >
</p>



			<br />
<p>
	<b>3.“Renesas cup”National Undergraduate Electronics Design Contest，Provincial Second Prize, 2013.</b>
	<br />
</p>

			<p align=justify">
				<b>4. RoboMaster Robotics Competition，Provincial Third Prize, 2016.</b>

				<a href="https://v.youku.com/v_show/id_XMzA4Nzg1NjE4OA==.html?spm=a2hzp.8244740.0.0"><img border="0" src="images/yt.png" height="13px"/> [video]</a>
				<br /><img border="0" src="images/rw2.jpg" align="middle"  width="400px" >
			</p>

		</div>

		<div id="footer">

			<hr />

	Copyright 2019 | JohnnyWang | All Rights Reserved

		</div>


	
</div>

</body>

</html>
